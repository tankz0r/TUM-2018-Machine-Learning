\documentclass[]{report}
\usepackage{amsmath}

% Title Page
\title{}
\author{}


\begin{document}
\maketitle

{\large\textbf{Problem 4}}
\\

Let's recall distributions of Gaussian and Gamma:  
\begin{align*}
N(x|\mu,\beta^{-1}))=\frac{\beta}{(2\pi)^{\frac{1}{2}}}\exp({\frac{-\beta(x-\mu)^{2}}{2}}) \\
Gamma(\lambda|a, b) = \frac{1}{\Gamma(a)}b^{a}\lambda^{a-1}\exp({-b\lambda}) \\
p(y|\Phi, w, \beta) = \prod_{i=1}^{N}N(y_i|w^{T}\phi(x_i),\beta^{-1})) \\
p(w, \beta) = N(w|m_0, \beta^{-1}S_0)Gamma(\beta|a_0,b_0) \\
\end{align*}

\begin{multline*}
p(w, \beta|D) \propto p(y|\Phi, w, \beta)p(w,\beta) = \\ \prod_{i=1}^{N}N(y_i|w^{T}\phi(x_i),\beta^{-1}))p(w, \beta) N(w|m_0, \beta^{-1}S_0)Gamma(\beta|a_0,b_0) = \\
\prod_{i=1}^{N}\frac{\beta}{(2\pi)^{\frac{1}{2}}}\exp({\frac{-\beta(y_i-w^{T}\phi(x_i))^{2}}{2}})
(\frac{\beta}{2\pi})^{\frac{D}{2}}\frac{1}{\det(S_0)}\exp({\frac{-(w-m_0)^{T}\beta^{-1}S_0(w-m_0)}{2}}) \times \\
\frac{1}{\Gamma(a_0)}b_0^{a_0}\beta^{a_0-1}\exp(-\beta b_0)
\end{multline*}
Let's use $\log$ to simplify computations:  

\begin{multline}
\label{1}
\log p(w, \beta|D) = -\frac{N}{2}\log(2\pi)+\frac{N}{2}\log{\beta}-\frac{\beta}{2}\sum_{i=1}^{n}(y_i-w^{T}\phi(x_i))^{2}- \\
\frac{D}{2}\log(2\pi)+\frac{1}{2}\log(\beta)-\frac{1}{2}\log(\det(S_0))-\frac{1}{2}(w-m_0)^{T}(\beta^{-1}S_0)^{-1}(w-m_0)+\log(\frac{1}{\Gamma(a_0)}b_0^{a_0})-b_0\beta+(a_0-1)\log(\beta)
\end{multline}

Now we will use techniques which were explained in Chapter 2.3.1 and 2.3.3 (Conditional Gaussian distribution and Bayesâ€™ theorem for Gaussian variables) Pattern Recognition and Machine Learning. 

\begin{align*}
-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu) = -\frac{1}{2}x^{T}\Sigma^{-1}x+x^{T}\Sigma^{-1}\mu+const 
\end{align*}

Given:

\begin{align*}
p(x) = N(x|\mu, \Lambda^{-1}) \\
p(y|x) = N(y|Ax+b, L^{-1})
\end{align*}

We can find: 

\begin{align*}
p(y) = N(y|A\mu+b, L^{-1}+A\Lambda^{-1}A^{T}) \\
p(x|y) = N(x|\Sigma(A^TL(y-b)+\Lambda\mu), \Sigma) \\
where  \Sigma = (\Lambda + A^TLA)^{-1}
\end{align*}

Now, we apply this technique to equation (\ref{1}), but first we can rewrite probability $p(w,\beta|D)$ using Product rule as follows: $p(w, \beta|t)=p(w|\beta, D)p(\beta|D)$

We group terms which have only w random variable(quadratic and linear).
Quadratic:

\begin{equation*}
-\frac{\beta}{2}w^{T}S_0^{-1}w - \frac{\beta}{2}w^{T}\Phi^{T}\Phi w+const = -\frac{\beta}{2}w^{T}(S_0^{-1}+\Phi^{T}\Phi)w + const
\end{equation*}

Linear:

\begin{equation*}
\beta w^{T}S_0^{-1}m_0 + \beta w^{T}\Phi y = \beta w^{T}(S_0^{-1}m_0 + \Phi y)
\end{equation*}

We got following:

\begin{align*}
p(w|\beta,D) = N(w|m_N, S_N) \\
S_N = (S_0^{-1}+\Phi^{T}\Phi)^{-1} \\
m_N = S_N(S_0^{-1}m_0 + \Phi y) \\
\end{align*}

Similarly for $\beta$: group terms which we did not use for w:
\begin{align*}
\log(p(\beta|D)) = \frac{N}{2}\log(\beta) - \frac{\beta}{2}m_0^{T}S_0^{-1}m_0 - b_0\beta + (a_0 - 1)\log{\beta} - \frac{\beta}{2}\sum_{i=1}^{N}(y_i)^{2}
\end{align*}

\begin{align*}
p(\beta|D) = Gamma(\beta|a_N, b_N) \\
a_N = a_0 + \frac{N}{2} \\
b_N = b_0 + \frac{1}{2}(m_0S_0^{-1}m_0 + yy^{T})
\end{align*}

As a result, posterior distribution takes the same form as the prior.


\end{document}          
