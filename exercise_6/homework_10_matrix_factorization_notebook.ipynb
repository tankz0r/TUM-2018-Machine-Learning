{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 10: Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.010051Z",
     "start_time": "2019-01-12T17:53:26.095019Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exporting the results to PDF\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook.\n",
    "2. Download the notebook in HTML (click File > Download as > .html)\n",
    "3. Convert the HTML to PDF using e.g. https://www.sejda.com/html-to-pdf or `wkhtmltopdf` for Linux ([tutorial](https://www.cyberciti.biz/open-source/html-to-pdf-freeware-linux-osx-windows-software/))\n",
    "4. Concatenate your solutions for other tasks with the output of Step 3. On a Linux machine you can simply use `pdfunite`, there are similar tools for other platforms too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "This way is preferred to using `nbconvert`, since `nbconvert` clips lines that exceed page width and makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant recommendation\n",
    "\n",
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not yet rated based on a latent factor model.\n",
    "\n",
    "Specifically, the objective function (loss) we wanted to optimize is:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(i, x) \\in W} (M_{ix} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(i, x)$ pairs for which the rating $M_{ix}$ given by user $i$ to restaurant $x$ is known. Here we have also introduced two regularization terms to help us with overfitting where $\\lambda$ is hyper-parameter that control the strength of the regularization.\n",
    "\n",
    "**Hint 1**: Using the closed form solution for regression might lead to singular values. To avoid this issue perform the regression step with an existing package such as scikit-learn. It is advisable to use ridge regression to account for regularization.\n",
    "\n",
    "**Hint 2**: If you are using the scikit-learn package remember to set ``fit intercept = False`` to only learn the coeficients of the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data (nothing to do here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.037742Z",
     "start_time": "2019-01-12T17:53:27.015266Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings = np.load(\"../datasets/ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.140170Z",
     "start_time": "2019-01-12T17:53:27.041687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset. We store the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.369585Z",
     "start_time": "2019-01-12T17:53:27.147832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = np.max(ratings[:,0] + 1)\n",
    "n_restaurants = np.max(ratings[:,1] + 1)\n",
    "M = sp.coo_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])), shape=(n_users, n_restaurants)).tocsr()\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the <a href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"> cold start problem</a>, in the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings.\n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.\n",
    "\n",
    "**Note**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. We store the indices for which we the rating data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.386465Z",
     "start_time": "2019-01-12T17:53:27.374982Z"
    }
   },
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    shape = (-1, -1)\n",
    "    while matrix.shape != shape:\n",
    "        shape = matrix.shape\n",
    "        nnz = matrix>0\n",
    "        row_ixs = nnz.sum(1).A1 > min_entries\n",
    "        matrix = matrix[row_ixs]\n",
    "        nnz = matrix>0\n",
    "        col_ixs = nnz.sum(0).A1 > min_entries\n",
    "        matrix = matrix[:,col_ixs]\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement a function that substracts the mean user rating from the sparse rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.521269Z",
     "start_time": "2019-01-12T17:53:27.389360Z"
    }
   },
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "      \n",
    "    tot = np.array(matrix.sum(axis=1).squeeze())[0]\n",
    "    cts = np.diff(matrix.indptr)\n",
    "    user_means = tot/cts\n",
    "    d = sp.diags(user_means, 0)\n",
    "    b = matrix.copy()\n",
    "    b.data = np.ones_like(b.data)\n",
    "    matrix = matrix - d*b\n",
    "    \n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.627784Z",
     "start_time": "2019-01-12T17:53:27.523856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 5], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sp.csr_matrix([[1., 0., 2.], [1.,2.,3.]])\n",
    "a.indptr #???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train, validation and test set (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:27.724407Z",
     "start_time": "2019-01-12T17:53:27.632097Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix_cp = matrix.copy()\n",
    "    non_zero_idx = np.argwhere(matrix_cp)\n",
    "    ixs = np.random.permutation(non_zero_idx)\n",
    "    val_idx = tuple(ixs[:n_validation].T)\n",
    "    test_idx = tuple(ixs[n_validation:n_validation + n_test].T)\n",
    "    \n",
    "    val_values = matrix_cp[val_idx].A1\n",
    "    test_values = matrix_cp[test_idx].A1\n",
    "    \n",
    "    matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "    matrix_cp.eliminate_zeros()\n",
    "\n",
    "    return matrix_cp, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:28.013789Z",
     "start_time": "2019-01-12T17:53:27.727941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (3529, 2072)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:28.061110Z",
     "start_time": "2019-01-12T17:53:28.017428Z"
    }
   },
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:28.138120Z",
     "start_time": "2019-01-12T17:53:28.065657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove user means.\n",
    "nonzero_indices = np.argwhere(M_train)\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[np.array(val_idx).T[:,0]]\n",
    "# .A1\n",
    "test_values_shifted = test_values - user_means[np.array(test_idx).T[:,0]]\n",
    "# .A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the loss function (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:28.247156Z",
     "start_time": "2019-01-12T17:53:28.141811Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(values, ixs, Q, P, reg_lambda):\n",
    "    \"\"\"\n",
    "    Compute the loss of the latent factor model (at indices ixs).\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : np.array, shape [n_ixs,]\n",
    "        The array with the ground-truth values.\n",
    "    ixs : tuple, shape [2, n_ixs]\n",
    "        The indices at which we want to evaluate the loss (usually the nonzero indices of the unshifted data matrix).\n",
    "    Q : np.array, shape [N, k]\n",
    "        The matrix Q of a latent factor model.\n",
    "    P : np.array, shape [k, D]\n",
    "        The matrix P of a latent factor model.\n",
    "    reg_lambda : float\n",
    "        The regularization strength\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "           The loss of the latent factor model.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_sse_loss = np.sum((values - Q.dot(P)[ixs])**2)\n",
    "    regularization_loss =  reg_lambda * (np.sum(np.linalg.norm(P, axis=0)**2) + np.sum(np.linalg.norm(Q, axis=1) ** 2))\n",
    "    \n",
    "    return mean_sse_loss + regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating optimization\n",
    "\n",
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement a function that initializes the latent factors $Q$ and $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:50.217657Z",
     "start_time": "2019-01-12T17:53:50.200502Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, 'random' means we initialize\n",
    "             the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    n_users, n_restaurants = matrix.shape\n",
    "    \n",
    "    if init == \"random\":\n",
    "        Q = sp.random(n_users, k)\n",
    "        P = sp.random(k, n_restaurants)\n",
    "    elif init == \"svd\":\n",
    "        Q, S, P = svds(matrix, k)\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implement the alternating optimization approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:55:48.830750Z",
     "start_time": "2019-01-12T17:55:48.803680Z"
    }
   },
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=5, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 5\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def als_step(latent_vectors, fixed_vecs, ratings, _lambda):\n",
    "        #||y - Xw||^2_2 + alpha * ||w||^2_2\n",
    "        clf = Ridge(alpha = _lambda, fit_intercept = False)\n",
    "        clf.fit(fixed_vecs, ratings)\n",
    "        latent_vectors = clf.predict(fixed_vecs)\n",
    "        return latent_vectors\n",
    "    \n",
    "    best_Q = None\n",
    "    best_P = None\n",
    "    validation_losses = []\n",
    "    train_losses = []\n",
    "    converged_after = None\n",
    "    Q, P = initialize_Q_P(M, k, init='random')\n",
    "    \n",
    "    ctr = 1\n",
    "    wait = 0\n",
    "    while ctr <= max_steps:\n",
    "        if ctr % log_every == 0:\n",
    "            train_loss = loss(M, non_zero_idx, Q, P, reg_lambda)\n",
    "            val_loss = loss(val_values, val_idx, Q, P, reg_lambda)\n",
    "            print('Iteration {}, training loss: {}, validation loss: {}'.format(ctr, train_loss, val_loss))\n",
    "            train_losses.append(train_loss)\n",
    "            validation_losses.append(val_loss)\n",
    "        \n",
    "        if validation_losses[-1] & validation_losses[-2]:\n",
    "            if (validation_losses[-2] - validation_losses[-1]) > 0:\n",
    "                best_P = P\n",
    "                best_Q = Q\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait +=1\n",
    "        \n",
    "        if wait > patience:\n",
    "            return best_Q, best_P\n",
    "        \n",
    "        Q = als_step(Q, P, M, reg_lambda)\n",
    "        P = als_step(P, Q, M, reg_lambda)\n",
    "        ctr += 1\n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the latent factor (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, P, val_loss, train_loss, converged = latent_factor_alternating_optimization(M_shifted, non_zero_indices, \n",
    "                                                                               k=100, val_idx=val_idx,\n",
    "                                                                               val_values=val_values_shifted, \n",
    "                                                                               reg_lambda=1e-4, init='random',\n",
    "                                                                               max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the validation and training losses over for each iteration (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T17:53:29.299730Z",
     "start_time": "2019-01-12T17:53:26.222Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[10, 5])\n",
    "fig.suptitle(\"Alternating optimization, k=100\")\n",
    "\n",
    "ax[0].plot(train_loss[1::])\n",
    "ax[0].set_title('Training loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "ax[1].plot(val_loss[1::])\n",
    "ax[1].set_title('Validation loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
